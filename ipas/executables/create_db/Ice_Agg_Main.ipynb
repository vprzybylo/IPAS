{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import ipas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sqlite3 import Connection as SQLite3Connection\n",
    "from sqlalchemy import create_engine, event, select\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import base\n",
    "from ipas import lab_ice_agg_SQL as lab\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "import cloudpickle as pickle\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(\n",
    "    queue='kratos',\n",
    "    walltime='04-23:00:00',\n",
    "    cores=1,\n",
    "    memory='7168MiB', #1 GiB = 1,024 MiB\n",
    "    processes=1)\n",
    "\n",
    "\n",
    "cluster.scale(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "phioarr=np.logspace(-2, 2, num=20, dtype=None)#just columns (0,2); plates (-2,0)\n",
    "#phioarr = phioarr[1:]\n",
    "lenphio = len(phioarr)\n",
    "\n",
    "reqarr = [50,60,70,80]\n",
    "numaspectratios=len(phioarr)\n",
    "ch_dist='gamma'         #anything other than gamma uses the characteristic from the best distribution pdf (lowest SSE)\n",
    "nclusters = 300        #changes how many aggregates per aspect ratio to consider\n",
    "ncrystals = 50       #number of monomers per aggregate 1\n",
    "minor = 'depth'        #'minorxy' from fit ellipse or 'depth' to mimic IPAS in IDL\n",
    "rand_orient = True    #randomly orient the seed crystal and new crystal: uses first random orientation\n",
    "save_plots = False     #saves all histograms to path with # of aggs and minor/depth folder\n",
    "file_ext = 'eps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    " \n",
    "    '''\n",
    "    for phio in phioarr:\n",
    "        for r in reqarr:\n",
    "            b1 = lab.collect_clusters(phio, r, nclusters=nclusters,\n",
    "                                ncrystals=ncrystals, rand_orient=rand_orient)     \n",
    "   \n",
    "    output = []\n",
    "    for r in reqarr:\n",
    "        for phio in phioarr:\n",
    "    \n",
    "            print('eq. vol rad', r, phio)\n",
    "            output.append(delayed(lab.collect_clusters)(phio, r, nclusters=nclusters,\n",
    "                                                        ncrystals=ncrystals, rand_orient=rand_orient))\n",
    "    \n",
    "    print(output)\n",
    "    print('computing...')\n",
    "    b1 = client.compute(output) \n",
    "    b1 = client.gather(b1)\n",
    "    '''\n",
    "\n",
    "    notebook=3\n",
    "    print(notebook)\n",
    "    for r in reqarr:\n",
    "        #b3 = []\n",
    "        print('r = ',r)\n",
    "        count = 1\n",
    "\n",
    "        pool = Pool(processes=20) #pools are reusable\n",
    "        parallel_clus=partial(lab.collect_clusters, notebook=notebook, r=r, nclusters=nclusters,\\\n",
    "                                ncrystals=ncrystals, \\\n",
    "                                rand_orient=rand_orient)\n",
    "\n",
    "        start = time.time()\n",
    "        output = pool.imap_unordered(parallel_clus, phioarr)\n",
    "        for done in output:\n",
    "            print(\"after %3.1fsec: count:%s\"%(time.time()-start, count))\n",
    "            count +=1\n",
    "\n",
    "#         for res in output:\n",
    "#             print(\"(after %3.1fsec)  mono phi:%.3f  count:%s\"%(time.time()-start, res[0].mono_phi, count))\n",
    "#             count += 1\n",
    "        print('closing')\n",
    "        pool.close()\n",
    "        print('joining')\n",
    "        pool.join()\n",
    "\n",
    "        \n",
    "        print('new r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    %time main() \n",
    "    \n",
    "#     filename = 'instance_3radii_iceagg_allrand_50xtalstot_20phi_rall_300agg_lastmono'\n",
    "#     filehandler = open(filename, 'wb')\n",
    "#     %time pickle.dump(b, filehandler)\n",
    "#     filehandler.close()\n",
    "#     print('finished, creating engine 1!')\n",
    "    \n",
    "#     engine = create_engine('sqlite:///IPAS_lastmono.sqlite')\n",
    "#     event.listen(engine, 'connect', _set_sqlite_pragma)\n",
    "#     base.Base.metadata.create_all(engine, checkfirst=True)\n",
    "#     Session = sessionmaker(bind=engine)\n",
    "#     session = Session()\n",
    "    \n",
    "#     try:\n",
    "#         for r in b:\n",
    "#             for obj in r:\n",
    "#                 session.add_all(obj)  # crystal id has been appended into cluster relationship\n",
    "#                 session.commit()\n",
    "\n",
    "#     except:\n",
    "#         print('in except')\n",
    "#         raise\n",
    "#     session.close() \n",
    "#     print('DONE!')\n",
    "    \n",
    "    #packed = msgpack.writeResult('msgpack_test', b1)\n",
    "    #msgpack.unpackb(packed, encoding='utf-8')\n",
    "    \n",
    "    #filename = 'instance_3radii_iceagg_allrand_100xtalstot_20phi_r10_1000agg'\n",
    "    #filehandler = open(filename, 'wb')\n",
    "    #%time pickle.dump(b1, filehandler)\n",
    "    #filehandler.close()\n",
    "    #print('finished!')\n",
    "    \n",
    "    #CHANGE LOOKUP TABLE FILES TO HAVE RANGE OF PHI AND R\n",
    "    #Kratos = 28*4nodes = 112 -> 28 req so 4 aspect ratios at a time\n",
    "    #would take 5 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_engines(eng_name):\n",
    "    engine=create_engine(eng_name)\n",
    "    db = 'SELECT * FROM aggregates'\n",
    "    df = pd.read_sql(db, con=engine)\n",
    "    return df\n",
    "\n",
    "# databases = ['sqlite:///IPAS.sqlite', 'sqlite:///IPAS_1.sqlite', 'sqlite:///IPAS_2.sqlite',\\\n",
    "#             'sqlite:///IPAS_3.sqlite', 'sqlite:///IPAS_4.sqlite', 'sqlite:///IPAS_5.sqlite']\n",
    "\n",
    "databases = ['sqlite:///IPAS.sqlite', 'sqlite:///IPAS_1.sqlite']\n",
    "\n",
    "pool = Pool(processes=2)\n",
    "df_all = pool.map(create_engines, databases)\n",
    "pool.close()\n",
    "\n",
    "# engine = create_engine('sqlite:///IPAS.sqlite')\n",
    "# engine1 = create_engine('sqlite:///IPAS_1.sqlite')\n",
    "# engine2 = create_engine('sqlite:///IPAS_2.sqlite')\n",
    "# engine3 = create_engine('sqlite:///IPAS_3.sqlite')\n",
    "# engine4 = create_engine('sqlite:///IPAS_4.sqlite')\n",
    "# engine5 = create_engine('sqlite:///IPAS_5.sqlite')\n",
    "\n",
    "# db = 'SELECT * FROM aggregates'\n",
    "# df = pd.read_sql(db, con=engine)\n",
    "# df1 = pd.read_sql(db, con=engine1)\n",
    "# df2 = pd.read_sql(db, con=engine2)\n",
    "# df3 = pd.read_sql(db, con=engine3)\n",
    "# df4 = pd.read_sql(db, con=engine4)\n",
    "# df5 = pd.read_sql(db, con=engine5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df = dd.read_sql_table('aggregates', 'sqlite:///IPAS.sqlite',\\\n",
    "                       npartitions=10, index_col='id').persist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_1 = dd.read_sql_table('aggregates', 'sqlite:///IPAS_1.sqlite',\\\n",
    "                       npartitions=28, index_col='id').persist()\n",
    "progress(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_2 = dd.read_sql_table('aggregates', 'sqlite:///IPAS_2.sqlite',\\\n",
    "                       npartitions=28, index_col='id')\n",
    "df_2.persist() \n",
    "progress(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%time df_3 = dd.read_sql_table('aggregates', 'sqlite:///IPAS_3.sqlite',\\\n",
    "                       npartitions=28, index_col='id')\n",
    "df_3.persist() \n",
    "progress(df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.persist() \n",
    "progress(df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_4 = dd.read_sql_table('aggregates', 'sqlite:///IPAS_4.sqlite',\\\n",
    "                       npartitions=28, index_col='id')\n",
    "df_4.persist() \n",
    "progress(df_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_5 = dd.read_sql_table('aggregates', 'sqlite:///IPAS_5.sqlite',\\\n",
    "                       npartitions=28, index_col='id')\n",
    "df_5.persist() \n",
    "progress(df_5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
