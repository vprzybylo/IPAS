{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Aggregation Script - calls lab.py and crystals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reloads the lab.py and crystals.py modules to update any changes (after saving)\n",
    "#If a new method or object is created, autoreload doesn't work and the \n",
    "#kernel needs to be closed and halted after saving and making a 'checkpoint'\n",
    "#in this notebook\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipas \n",
    "import numpy as np\n",
    "import dask\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client, progress\n",
    "from dask import delayed\n",
    "from dask import dataframe as dd\n",
    "import functools\n",
    "import sys\n",
    "import ast\n",
    "from struct import *\n",
    "import pickle\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from dask.distributed import as_completed\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/network/rit/lab/sulialab/share/bin/miniconda3/envs/pangeo/lib/python3.6/site-packages/distributed/dashboard/core.py:72: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    }
   ],
   "source": [
    "cluster = SLURMCluster(\n",
    "    queue='kratos',\n",
    "    walltime='04-23:00:00',\n",
    "    cores=1,\n",
    "    memory='20000MiB', #1 GiB = 1,024 MiB\n",
    "    processes=1)\n",
    "\n",
    "#cluster.adapt(minimum=3, maximum=20)\n",
    "cluster.scale(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://169.226.65.49:46513</li>\n",
       "  <li><b>Dashboard: </b><a href='http://169.226.65.49:43069/status' target='_blank'>http://169.226.65.49:43069/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>41.94 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://169.226.65.49:46513' processes=2 threads=2, memory=41.94 GB>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize databases for queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['sqlite:///'+f for f in glob.glob(\"db_files/IPAS_*_lastmono.sqlite\")]\n",
    "tables = ['aggregates', 'crystals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with compute\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df=[]\n",
    "for table in tables:\n",
    "    \n",
    "    #read tables in parallel on client \n",
    "    read_files = [dask.delayed(dd.read_sql_table)(table=table, uri=file, index_col='id') for file in files]\n",
    "    \n",
    "    compute_read = client.compute(read_files)\n",
    "    print('done with compute')\n",
    "    ddfs = client.gather(compute_read)\n",
    "    print('done with gather')\n",
    "    #concatenate all sqlite files vertically (axis=0 default) (same columns)\n",
    "    gathered_reads = client.scatter(ddfs)\n",
    "    ddf = client.submit(dd.concat, gathered_reads).result()\n",
    "    print('done with submit')\n",
    "    #append combined dask df for each table\n",
    "    df.append(ddf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_concat = dd.concat([df[0], df[1]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.agg_r = np.power((np.power(df_concat.a, 2) * df_concat.c), (1./3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_r_5000(df):\n",
    "    return df[df.agg_r < 5000]\n",
    "\n",
    "df_concat = df_concat.map_partitions(query_r_5000)\n",
    "#len(df_concat) #86% of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repart = df_concat.repartition(partition_size=\"100MB\").persist()\n",
    "df_repart.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ch_dist='gamma'         #anything other than gamma uses the characteristic from the best distribution pdf (lowest SSE)\n",
    "rand_orient = True  #randomly orient the seed crystal and new crystal: uses first random orientation\n",
    "save_plots = False     #saves all histograms to path with # of aggs and minor/depth folder\n",
    "\n",
    "#Note, there may be a shape parameter erlang distribution run time warning occasionally if warning filter\n",
    "#is turned off, disregard it\n",
    "#warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_points_all(agg):\n",
    "\n",
    "    ncrystals = agg.ncrystals    \n",
    "    #print('ncrystals', ncrystals)\n",
    "    agg_id = agg.agg_id\n",
    "    #print('ncrystals, phi, r, agg_id', ncrystals, agg_id, agg_id-ncrystals)\n",
    "    \n",
    "    query = df_repart[(df_repart.r == agg.r) & (df_repart.phi == agg.phi) & \\\n",
    "                     (df_repart.ncrystals >= 2) & (df_repart.ncrystals <= ncrystals) &\\\n",
    "                     (df_repart.agg_id <= agg_id) & (df_repart.agg_id >= agg_id-ncrystals)].compute()\n",
    "    \n",
    "    cluster = ipas.Cluster_Calculations(agg)\n",
    "    hold_points = []\n",
    "    for crys in query.itertuples():\n",
    "        for points in pickle.loads(crys.points):\n",
    "            hold_points.append(points)\n",
    "        #print('hold points', hold_points)\n",
    "\n",
    "    #cluster.points = np.concatenate(hold_points)\n",
    "    cluster.points = np.reshape(hold_points, (int(np.shape(hold_points)[0]/12), 12))\n",
    "    cluster.points = np.array(cluster.points, dtype=[('x', float), ('y', float), ('z', float)])        \n",
    "        \n",
    "#     points = np.concatenate(hold_points)\n",
    "#     points = np.reshape(points, (int(np.shape(cluster.points)[0]/12), 12))\n",
    "#     points = np.array(points, dtype=[('x', float), ('y', float), ('z', float)])    \n",
    "  \n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    output = []\n",
    "    hold_clusters  = np.empty((20,20,300), dtype=object)\n",
    "    hold_monos = np.empty((20,20,300), dtype=object)\n",
    "    \n",
    "    #pull 300 monomer samples per r, phi:\n",
    "    phio_m=np.logspace(-2, 2, num=2, dtype=None)#just columns (0,2); plates (-2,0)\n",
    "    #req_m = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100,200,300,400,500,600,700,800,900,1000]\n",
    "    req_m = [1,2,3]\n",
    "    \n",
    "    for j in range(len(phio_m)):  #mono phi\n",
    "        for k in range(len(req_m)):   #mono r\n",
    "            df_mono_phi = df_repart[(df_repart.phi == phio_m[j]) & (df_repart.r == req_m[k])].compute()\n",
    "            samples_mono = df_mono_phi.sample(3)\n",
    "            \n",
    "            n_monos=0\n",
    "            for mono in samples_mono.itertuples():\n",
    "                mono = ipas.Cluster_Calculations(mono)\n",
    "                mono.points = np.array(pickle.loads(mono.points), dtype=[('x', float), ('y', float), ('z', float)])        \n",
    "                mono.agg_r = None\n",
    "                mono.agg_phi = None\n",
    "                mono.ncrystals = 1\n",
    "                mono.a = ((mono.r*2)**3./phio_m[j])**(1./3.)\n",
    "                mono.b = 2*((mono.a/2.)*np.sin(60))\n",
    "                mono.c = phio_m[j]*mono.a\n",
    "                hold_monos[j,k,n_monos] = mono\n",
    "                n_monos+=1\n",
    "    \n",
    "            res, phi_bins = pd.qcut(df_repart.agg_phi.compute(), 1, retbins=True)\n",
    "            print(phi_bins)\n",
    "\n",
    "            for i in range(len(phi_bins)-1):  #agg phi\n",
    "\n",
    "                #print('phi_bin = ', phi_bins[i], phi_bins[i+1])\n",
    "                #return a df that only queries within an aspect ratio bin\n",
    "                df_phi = df_repart[(df_repart.agg_phi > phi_bins[i]) & (df_repart.agg_phi < phi_bins[i+1]) & \\\n",
    "                                  (df_repart.ncrystals > 2)]  #to ensure at least 2 crystals within agg since ncrystals=1 not in db\n",
    "                #now break that aspect ratio bin into 20 equal r bins\n",
    "                res, r_bins = pd.qcut(df_phi.agg_r.compute(), 1, retbins=True)\n",
    "                print(r_bins)\n",
    "\n",
    "                for r in range(len(r_bins)-1):   #agg r\n",
    "                    print('i, r ',i, r)\n",
    "\n",
    "                    print('r = ', r_bins[r], r_bins[r+1])\n",
    "                    df_r = df_phi[(df_phi.agg_r > r_bins[r]) & (df_phi.agg_r < r_bins[r+1]) &\\\n",
    "                                    (df_phi.ncrystals > 2)].compute() \n",
    "\n",
    "                    #print(df_repart.id.value_counts().compute().head(30))         \n",
    "\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    samples = df_r.sample(3)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    hold_clus = []\n",
    "                    n_aggs=0\n",
    "                    for agg in samples.itertuples():\n",
    "                        cluster = concatenate_points_all(agg)\n",
    "                        #print(cluster.points)\n",
    "                        hold_clus.append(cluster)\n",
    "                        hold_clusters[i,r,n_aggs] = cluster\n",
    "                        n_aggs+=1\n",
    "                    print('time to concatenate all pts = ', (time.time()-start_time))\n",
    "\n",
    "        #             delayeds = []\n",
    "        #             for agg in samples.itertuples():\n",
    "        #                 delayeds.append(dask.delayed(concatenate_points_all)(agg))\n",
    "        #             delayeds = client.compute(delayeds)\n",
    "        #             hold_clusters[i,r,:] = client.gather(delayeds)\n",
    "        #             print('time to concatenate all pts = ', (time.time()-start_time))\n",
    "\n",
    "                    #output.append(dask.delayed(ipas.collect_clusters)(hold_clus, hold_monos, rand_orient=rand_orient)) \n",
    "\n",
    "#     start_time = time.time()\n",
    "#     output = client.compute(output)\n",
    "#     output = client.gather(output)\n",
    "#     print('time to collect = ', (time.time()-start_time))\n",
    "#     print('done gathering!')\n",
    "                    %time output.append(ipas.collect_clusters(hold_clus, hold_monos, rand_orient=rand_orient))\n",
    "    \n",
    "    return output, hold_clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    output, hold_clusters = main() \n",
    "    print(output)\n",
    "   \n",
    "#     filename = 'instance_files/pulled_clusters_rand_iceagg'\n",
    "#     filehandler = open(filename, 'wb')\n",
    "#     pickle.dump(hold_clusters, filehandler)\n",
    "#     filehandler.close()\n",
    "#     print('finished!')\n",
    "    \n",
    "#     filename = 'instance_files/instance_db_iceagg_rand_returnclus'\n",
    "#     filehandler = open(filename, 'wb')\n",
    "#     pickle.dump(output, filehandler)\n",
    "#     filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
